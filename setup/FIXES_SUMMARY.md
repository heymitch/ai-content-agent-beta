# ðŸŽ¯ Complete Fix for Client Deployment Issues

## TL;DR

**Problem:** Clients' n8n workflows fail at "Delete Old Data Rows" node because tables don't exist.

**Root Cause:** Clients had to manually "run once" on 3 PostgreSQL nodes to create tables, but most failed or skipped them.

**Solution:**
1. Run `bootstrap_supabase.sql` to create ALL tables automatically
2. Remove the 3 manual "run once" PostgreSQL nodes from workflow
3. Fix the filter syntax in "Delete Old Doc Rows" node (optional improvement)

---

## Why It Works for You But Not Clients

### Your Setup (Working):
```
âœ… You manually ran all 3 "Create Table" nodes successfully
âœ… Tables exist: company_documents, document_metadata, document_rows
âœ… Workflow can delete/insert without errors
```

### Client Setup (Broken):
```
âŒ Ran first "Create Table" node â†’ success
âŒ Tried to run it again â†’ "already exists" error â†’ gave up
âŒ Never ran the other 2 nodes
âŒ Missing tables: document_metadata, document_rows
âŒ Workflow fails when trying to DELETE from non-existent tables
```

---

## The Complete Fix

### Step 1: Bootstrap Database (Required)

This creates ALL tables automatically, no manual node clicking needed:

```bash
cd setup
./deploy_to_client.sh "postgresql://postgres.[PROJECT]:[PASSWORD]@[HOST]:5432/postgres"
```

**What this creates:**
- âœ… `company_documents` (for n8n vectorstore)
- âœ… `document_metadata` (for n8n metadata tracking)
- âœ… `document_rows` (for n8n tabular data)
- âœ… `content_examples` (for agent RAG)
- âœ… `research` (for agent research)
- âœ… `generated_posts` (for agent output)
- âœ… `slack_threads` (for agent sessions)
- âœ… `conversation_history` (for agent memory)
- âœ… `performance_analytics` (for tracking)

### Step 2: Clean Up n8n Workflow (Required)

Delete these 4 nodes from the workflow:

1. âŒ **"Create Documents Table and Match Function"** (PostgreSQL node)
2. âŒ **"Create Document Metadata Table"** (PostgreSQL node)
3. âŒ **"Create Document Rows Table (for Tabular Data)"** (PostgreSQL node)
4. âŒ **Sticky Note: "Run Each Node Once to Set Up Database Tables"**

**Why?** Bootstrap script already created everything.

### Step 3: Fix Delete Node Filter (Recommended)

**Optional but recommended** to prevent future issues.

Find the **"Delete Old Doc Rows"** Supabase node and update:

**Current filter:**
```
filterType: "string"
filterString: "metadata->>file_id=like.*{{ $json.file_id }}*"
```

**Better filter (Option A - Use metadata):**
```
filterType: "string"
filterString: "metadata->>'google_drive_file_id'=eq.{{ $('Set File ID').item.json.file_id }}"
```

**Or even better (Option B - Use direct column):**
```
filterType: "Manual"
Filters:
  - Key Name: google_drive_file_id
  - Condition: eq
  - Key Value: ={{ $('Set File ID').item.json.file_id }}
```

But for this to work, you'd need to ensure the `google_drive_file_id` column is populated. Currently, your workflow only populates the `metadata` JSONB field.

**Recommendation:** Use Option A (metadata filter fix) since it matches your current data structure.

---

## What Clients Need to Do

### 1. Get Database Credentials

From Supabase Dashboard â†’ Settings â†’ Database:

```
Connection String (URI):
postgresql://postgres.[PROJECT]:[PASSWORD]@aws-1-us-east-1.pooler.supabase.com:5432/postgres
```

### 2. Run Bootstrap Script

Send them this command:

```bash
# Download bootstrap script
curl -O https://[YOUR-REPO]/setup/bootstrap_supabase.sql

# Or if they have the repo:
cd ai-content-agent-template/setup
./deploy_to_client.sh "[THEIR-CONNECTION-STRING]"
```

### 3. Re-import n8n Workflow

Send them the updated workflow JSON with:
- 3 PostgreSQL nodes removed
- Fixed delete node filter (optional)

### 4. Configure Credentials

In n8n:
- Supabase API: Their URL + Service Key
- PostgreSQL: Their database connection string
- Google Drive: Their OAuth2
- OpenAI: Their API key

### 5. Test

Add a file to Google Drive â†’ Watch workflow run successfully.

---

## Verification Steps

### Check Tables Exist

```sql
SELECT table_name
FROM information_schema.tables
WHERE table_schema = 'public'
AND table_name IN (
  'company_documents',
  'document_metadata',
  'document_rows'
)
ORDER BY table_name;
```

Should return all 3 tables.

### Check RLS Policies

```sql
SELECT tablename, policyname
FROM pg_policies
WHERE tablename IN ('document_metadata', 'document_rows')
ORDER BY tablename;
```

Should show policies for both tables.

### Test Workflow

1. Upload test file to Google Drive folder
2. Watch n8n execution
3. Verify no errors on:
   - âœ… "Delete Old Doc Rows" (should delete 0 or N rows)
   - âœ… "Delete Old Data Rows" (should delete 0 or N rows)
   - âœ… "Insert Document Metadata" (should insert 1 row)
   - âœ… "Insert into Supabase Vectorstore" (should insert chunks)

---

## For Ross Specifically

Since you said the first PostgreSQL node already ran and failed on re-run:

### Quick Fix Option 1: Just Run Bootstrap

```bash
cd setup
./deploy_to_client.sh "postgresql://[YOUR-DB-URL]"
```

This will:
- Skip `company_documents` (already exists)
- Create `document_metadata` âœ…
- Create `document_rows` âœ…
- Add any missing indexes/policies

Then your workflow should work!

### Quick Fix Option 2: Run Missing SQL Manually

In Supabase SQL Editor, run:

```sql
-- Create document_metadata table
CREATE TABLE IF NOT EXISTS document_metadata (
    id TEXT PRIMARY KEY,
    title TEXT,
    url TEXT,
    created_at TIMESTAMP DEFAULT NOW(),
    schema TEXT
);

-- Create document_rows table
CREATE TABLE IF NOT EXISTS document_rows (
    id SERIAL PRIMARY KEY,
    dataset_id TEXT REFERENCES document_metadata(id) ON DELETE CASCADE,
    row_data JSONB
);

-- Add indexes
CREATE INDEX IF NOT EXISTS idx_document_metadata_created_at ON document_metadata(created_at DESC);
CREATE INDEX IF NOT EXISTS idx_document_rows_dataset_id ON document_rows(dataset_id);
CREATE INDEX IF NOT EXISTS idx_document_rows_data ON document_rows USING GIN (row_data);

-- Enable RLS
ALTER TABLE document_metadata ENABLE ROW LEVEL SECURITY;
ALTER TABLE document_rows ENABLE ROW LEVEL SECURITY;

-- Add policies
CREATE POLICY "Users can manage their own document metadata"
  ON document_metadata FOR ALL USING (true) WITH CHECK (true);

CREATE POLICY "Users can manage their own document rows"
  ON document_rows FOR ALL USING (true) WITH CHECK (true);
```

Then re-test your n8n workflow!

---

## For Nishant

You mentioned you "modified the first three nodes to get the data and database in that format."

**Don't modify the nodes!** Instead:

1. Run the bootstrap script (creates all tables correctly)
2. Remove those 3 PostgreSQL nodes entirely
3. Use the workflow as-is (it will work once tables exist)

If you modified the nodes and it's still not working, you likely have a schema mismatch. Easiest fix:

```sql
-- Drop the manually created tables
DROP TABLE IF EXISTS document_rows CASCADE;
DROP TABLE IF EXISTS document_metadata CASCADE;
DROP TABLE IF EXISTS company_documents CASCADE;
```

Then run the bootstrap script to recreate them correctly.

---

## Why This Happened

The original workflow design required manual "one-time setup" steps:

1. Click node 1 â†’ Create tables
2. Click node 2 â†’ Create more tables
3. Click node 3 â†’ Create even more tables

**Problems:**
- Confusing for clients
- Easy to miss steps
- Re-running causes "already exists" errors
- Hard to debug what's missing

**New approach:**
- One bootstrap script creates EVERYTHING
- No manual clicking
- Safe to run multiple times (`IF NOT EXISTS`)
- Clients just import workflow and go

---

## Summary Checklist

For you (developer):
- [x] Create `bootstrap_supabase.sql` with all tables
- [x] Create `deploy_to_client.sh` deployment script
- [ ] Update n8n workflow JSON (remove 3 PostgreSQL nodes)
- [ ] Update n8n workflow JSON (fix delete filter - optional)
- [ ] Export cleaned workflow for clients
- [ ] Update documentation

For clients:
- [ ] Get database connection string from Supabase
- [ ] Run bootstrap script
- [ ] Verify 9 tables exist
- [ ] Import updated n8n workflow
- [ ] Configure credentials (Supabase, Drive, OpenAI)
- [ ] Test workflow with sample file
- [ ] Celebrate! ðŸŽ‰
