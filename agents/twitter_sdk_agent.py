"""
Twitter SDK Agent - Tier 2 Orchestrator
Uses Claude Agent SDK with persistent memory and delegates to specialized tools.
Enforces quality thresholds: 18/25 minimum, targeting 80-90% human detection.
"""

from claude_agent_sdk import (
    ClaudeSDKClient,
    ClaudeAgentOptions,
    tool,
    create_sdk_mcp_server
)
import os
import json
import asyncio
from typing import Dict, Optional, Any, List
from datetime import datetime, timedelta
from dotenv import load_dotenv
from textwrap import dedent

# Load environment variables for API keys
load_dotenv()


# ================== TIER 3 TOOL DEFINITIONS (LAZY LOADED) ==================
# Tool descriptions kept minimal to reduce context window usage
# Detailed prompts loaded just-in-time when tools are called

@tool(
    "generate_5_hooks",
    "Generate 5 Twitter thread hooks",
    {"topic": str, "context": str, "target_audience": str}
)
async def generate_5_hooks(args):
    """Generate 5 hooks - prompt loaded JIT"""
    from anthropic import Anthropic
    from prompts.twitter_tools import GENERATE_HOOKS_PROMPT
    client = Anthropic(api_key=os.getenv('ANTHROPIC_API_KEY'))

    topic = args.get('topic', '')
    context = args.get('context', '')
    audience = args.get('target_audience', 'professionals')

    json_example = '[{{"type": "question", "text": "...", "chars": 45}}, ...]'
    prompt = GENERATE_HOOKS_PROMPT.format(
        topic=topic,
        context=context,
        audience=audience,
        json_example=json_example
    )

    response = client.messages.create(
        model="claude-sonnet-4-5-20250929",
        max_tokens=1000,
        messages=[{"role": "user", "content": prompt}]
    )

    return {
        "content": [{
            "type": "text",
            "text": response.content[0].text
        }]
    }


@tool(
    "inject_proof_points",
    "Add metrics and proof points",
    {"draft": str, "topic": str, "industry": str}
)
async def inject_proof_points(args):
    """Inject proof - prompt loaded JIT"""
    from anthropic import Anthropic
    from prompts.twitter_tools import INJECT_PROOF_PROMPT, WRITE_LIKE_HUMAN_RULES
    client = Anthropic(api_key=os.getenv('ANTHROPIC_API_KEY'))

    draft = args.get('draft', '')
    topic = args.get('topic', '')
    industry = args.get('industry', 'Tech')

    prompt = INJECT_PROOF_PROMPT.format(
        write_like_human_rules=WRITE_LIKE_HUMAN_RULES,
        draft=draft,
        topic=topic,
        industry=industry
    )

    response = client.messages.create(
        model="claude-sonnet-4-5-20250929",
        max_tokens=2000,
        messages=[{"role": "user", "content": prompt}]
    )

    return {
        "content": [{
            "type": "text",
            "text": response.content[0].text
        }]
    }


@tool(
    "create_human_draft",
    "Generate Twitter thread with quality self-assessment",
    {"topic": str, "hook": str, "context": str}
)
async def create_human_draft(args):
    """Create thread with JSON output including scores"""
    import json
    from anthropic import Anthropic
    from prompts.twitter_tools import CREATE_HUMAN_DRAFT_PROMPT
    client = Anthropic(api_key=os.getenv('ANTHROPIC_API_KEY'))

    topic = args.get('topic', '')
    hook = args.get('hook', '')
    context = args.get('context', '')

    # Lazy load prompt
    prompt = CREATE_HUMAN_DRAFT_PROMPT.format(
        topic=topic,
        hook=hook,
        context=context
    )

    response = client.messages.create(
        model="claude-sonnet-4-5-20250929",  # Use Sonnet for quality self-assessment
        max_tokens=3000,
        messages=[{"role": "user", "content": prompt}]
    )

    response_text = response.content[0].text

    # Try to parse JSON, fallback to raw text if it fails
    try:
        json_result = json.loads(response_text)
        # Validate schema - expect "tweets" array with char counts
        if "tweets" in json_result and "self_assessment" in json_result:
            # Validate each tweet is under 280 chars
            for tweet in json_result["tweets"]:
                if tweet.get("char_count", 0) > 280:
                    tweet["WARNING"] = f"Tweet exceeds 280 chars ({tweet['char_count']})"
            return {"content": [{"type": "text", "text": json.dumps(json_result, indent=2)}]}
    except json.JSONDecodeError:
        pass

    # Fallback: return raw text wrapped in basic JSON
    return {
        "content": [{
            "type": "text",
            "text": json.dumps({
                "tweets": [{"number": 1, "text": response_text, "char_count": len(response_text)}],
                "self_assessment": {"total": 0, "notes": "JSON parsing failed, returning raw text as single tweet"}
            }, indent=2)
        }]
    }


@tool(
    "validate_format",
    "Validate Twitter thread format",
    {"post": str, "post_type": str}
)
async def validate_format(args):
    """Validate format - prompt loaded JIT"""
    from anthropic import Anthropic
    from prompts.twitter_tools import VALIDATE_FORMAT_PROMPT
    client = Anthropic(api_key=os.getenv('ANTHROPIC_API_KEY'))

    post = args.get('post', '')
    post_type = args.get('post_type', 'standard')

    prompt = VALIDATE_FORMAT_PROMPT.format(
        post=post,
        post_type=post_type
    )

    response = client.messages.create(
        model="claude-sonnet-4-5-20250929",
        max_tokens=2000,
        messages=[{"role": "user", "content": prompt}]
    )

    return {
        "content": [{
            "type": "text",
            "text": response.content[0].text
        }]
    }


@tool(
    "search_viral_patterns",
    "Search for viral Twitter patterns in your niche",
    {"topic": str, "industry": str, "days_back": int}
)
async def search_viral_patterns(args):
    """Research what's working on Twitter right now"""
    topic = args.get('topic', '')
    industry = args.get('industry', '')
    days = args.get('days_back', 7)

    # This would connect to Twitter API or scraping service
    # For now, return strategic patterns

    patterns = {
        "trending_hooks": [
            "Declarative statements get 2x more engagement",
            "Threads with specifics get 45% more bookmarks",
            "Controversial takes increase quote tweets by 60%"
        ],
        "content_pillars": {
            "AI/Tech": ["AI agents", "Automation", "Dev tools"],
            "Business": ["Growth tactics", "Monetization", "Team building"],
            "Personal": ["Lessons learned", "Mindset", "Productivity"]
        },
        "winning_formats": [
            "Paragraph Style: Single declarative perspective",
            "What/How/Why: Hook ‚Üí 3-5 examples ‚Üí Insight",
            "Listicle: Opening sentence ‚Üí Bulleted list",
            "Old vs New: Contrast with mirrored structure",
            "10 Magical Ways: Tips/Steps/Lessons in numbered format"
        ],
        "engagement_triggers": [
            "Thoughts?",
            "What am I missing?",
            "Who else?",
            "Simple open-ended questions work best"
        ]
    }

    return {
        "content": [{
            "type": "text",
            "text": json.dumps(patterns, indent=2)
        }]
    }


@tool(
    "score_and_iterate",
    "Score thread quality",
    {"post": str, "target_score": int, "iteration": int}
)
async def score_and_iterate(args):
    """Score thread - prompt loaded JIT"""
    from anthropic import Anthropic
    from prompts.twitter_tools import SCORE_ITERATE_PROMPT
    client = Anthropic(api_key=os.getenv('ANTHROPIC_API_KEY'))

    post = args.get('post', '')
    target = args.get('target_score', 85)
    iteration = args.get('iteration', 1)

    prompt = SCORE_ITERATE_PROMPT.format(
        draft=post,
        target_score=target,
        iteration=iteration
    )

    response = client.messages.create(
        model="claude-sonnet-4-5-20250929",
        max_tokens=1500,
        messages=[{"role": "user", "content": prompt}]
    )

    return {
        "content": [{
            "type": "text",
            "text": response.content[0].text
        }]
    }


@tool(
    "quality_check",
    "Score thread on 5 axes and return surgical fixes",
    {"post": str}
)
async def quality_check(args):
    """Evaluate thread with 5-axis rubric + surgical feedback + Tavily search for fabrications"""
    import json
    from anthropic import Anthropic
    from prompts.twitter_tools import QUALITY_CHECK_PROMPT
    from tavily import TavilyClient

    client = Anthropic(api_key=os.getenv('ANTHROPIC_API_KEY'))
    tavily = TavilyClient(api_key=os.getenv('TAVILY_API_KEY'))

    post = args.get('post', '')
    prompt = QUALITY_CHECK_PROMPT.format(post=post)

    # Define Tavily search tool for Claude
    tavily_tool = {
        "name": "web_search",
        "description": "Search the web for current information to verify claims, names, companies, and news stories. Returns recent, relevant results.",
        "input_schema": {
            "type": "object",
            "properties": {
                "query": {
                    "type": "string",
                    "description": "Search query (e.g., 'Rick Beato YouTube AI filters', 'James Chen Clearbit')"
                }
            },
            "required": ["query"]
        }
    }

    messages = [{"role": "user", "content": prompt}]

    # Tool loop: handle Tavily search requests
    max_iterations = 5
    for iteration in range(max_iterations):
        response = client.messages.create(
            model="claude-sonnet-4-5-20250929",
            max_tokens=3000,
            tools=[tavily_tool],
            messages=messages
        )

        # Check if Claude wants to use a tool
        if response.stop_reason == "tool_use":
            # Extract tool use from response
            tool_use_block = next((block for block in response.content if block.type == "tool_use"), None)

            if tool_use_block and tool_use_block.name == "web_search":
                # Execute Tavily search
                query = tool_use_block.input["query"]
                try:
                    tavily_results = tavily.search(query, max_results=3)
                    search_output = "\n\n".join([
                        f"**{r.get('title', 'No title')}**\n{r.get('content', 'No content')}\nSource: {r.get('url', 'No URL')}"
                        for r in tavily_results.get('results', [])
                    ])
                except Exception as e:
                    search_output = f"Search error: {str(e)}"

                # Add assistant message with tool use, then tool result
                messages.append({
                    "role": "assistant",
                    "content": response.content
                })
                messages.append({
                    "role": "user",
                    "content": [{
                        "type": "tool_result",
                        "tool_use_id": tool_use_block.id,
                        "content": search_output
                    }]
                })
                # Continue loop to get final response
                continue

            # Unknown tool - shouldn't happen
            messages.append({
                "role": "assistant",
                "content": response.content
            })
            continue

        # Got final text response
        final_text = ""
        for block in response.content:
            if hasattr(block, 'text'):
                final_text += block.text

        # Try to parse JSON, return as-is if valid
        try:
            json_result = json.loads(final_text)
            if "scores" in json_result and "decision" in json_result:
                return {"content": [{"type": "text", "text": json.dumps(json_result, indent=2)}]}
        except json.JSONDecodeError:
            pass

        # Fallback: return raw text
        return {"content": [{"type": "text", "text": final_text}]}

    # Max iterations reached
    return {
        "content": [{
            "type": "text",
            "text": json.dumps({
                "scores": {"total": 0},
                "decision": "error",
                "issues": [],
                "surgical_summary": "Max iterations reached"
            }, indent=2)
        }]
    }


@tool(
    "apply_fixes",
    "Apply 3-5 surgical fixes based on quality_check feedback",
    {"post": str, "issues_json": str}
)
async def apply_fixes(args):
    """Apply surgical fixes without rewriting the whole thread"""
    import json
    from anthropic import Anthropic
    from prompts.twitter_tools import APPLY_FIXES_PROMPT, WRITE_LIKE_HUMAN_RULES
    client = Anthropic(api_key=os.getenv('ANTHROPIC_API_KEY'))

    post = args.get('post', '')
    issues_json = args.get('issues_json', '[]')

    # Use APPLY_FIXES_PROMPT with WRITE_LIKE_HUMAN_RULES
    prompt = APPLY_FIXES_PROMPT.format(
        post=post,
        issues_json=issues_json,
        write_like_human_rules=WRITE_LIKE_HUMAN_RULES
    )

    response = client.messages.create(
        model="claude-sonnet-4-5-20250929",  # Use Sonnet for surgical precision
        max_tokens=3000,
        messages=[{"role": "user", "content": prompt}]
    )

    response_text = response.content[0].text

    # Try to parse JSON
    try:
        json_result = json.loads(response_text)
        if "revised_thread" in json_result:
            return {"content": [{"type": "text", "text": json.dumps(json_result, indent=2)}]}
    except json.JSONDecodeError:
        pass

    # Fallback: return raw text wrapped in JSON
    return {
        "content": [{
            "type": "text",
            "text": json.dumps({
                "revised_thread": response_text,
                "changes_made": [],
                "estimated_new_score": 0,
                "notes": "JSON parsing failed"
            }, indent=2)
        }]
    }




# ================== TWITTER SDK AGENT CLASS ==================

class TwitterSDKAgent:
    """
    Tier 2: Twitter-specific SDK Agent with persistent memory
    Orchestrates Tier 3 tools and maintains platform-specific context
    """

    def __init__(self, user_id: str = "default", isolated_mode: bool = False):
        """Initialize Twitter SDK Agent with memory and tools

        Args:
            user_id: User identifier for session management
            isolated_mode: If True, creates isolated sessions (for testing only)
        """
        self.user_id = user_id
        self.sessions = {}  # Track multiple content sessions
        self.isolated_mode = isolated_mode  # Test mode flag

        # Twitter-specific base prompt with quality thresholds
        base_prompt = """You are a Twitter thread creation agent. Your goal: threads that score 18+ out of 25 without needing 3 rounds of revision.

YOU MUST USE TOOLS. EXECUTE immediately. Parse JSON responses.

AVAILABLE TOOLS (5-tool workflow):

1. mcp__twitter_tools__generate_5_hooks
   Input: {"topic": str, "context": str, "audience": str}
   Returns: JSON array with 5 hooks (question/bold/stat/story/mistake formats)
   When to use: Always call first to generate hook options for first tweet

2. mcp__twitter_tools__create_human_draft
   Input: {"topic": str, "hook": str, "context": str}
   Returns: JSON with {tweets: [{number, text, char_count}], self_assessment: {hook: 0-5, flow: 0-5, brevity: 0-5, proof: 0-5, cta: 0-5, total: 0-25}}
   What it does: Creates Twitter thread using 127-line WRITE_LIKE_HUMAN_RULES (cached)
   Quality: Trained on viral thread examples - produces human-sounding content
   Twitter-specific: Each tweet MUST be <280 chars, NO markdown, lowercase OK
   When to use: After selecting best hook from generate_5_hooks

3. mcp__twitter_tools__inject_proof_points
   Input: {"draft": str, "topic": str, "industry": str}
   Returns: Enhanced thread with metrics from topic/context (NO fabrication)
   What it does: Adds specific numbers/dates/names ONLY from provided context
   When to use: After create_human_draft, before quality_check

4. mcp__twitter_tools__quality_check
   Input: {"post": str}
   Returns: JSON with {scores: {hook/flow/brevity/proof/cta/total, ai_deductions}, decision: accept/revise/reject, issues: [{axis, severity, original, fix, impact}], searches_performed: [...], char_violations: [{tweet_num, char_count}]}
   What it does: 
   - Evaluates 5-axis rubric (0-5 each, total 0-25)
   - AUTO-DETECTS AI tells with -2pt deductions: contrast framing, rule of three, cringe questions
   - VALIDATES 280 char limit per tweet (returns char_violations if exceeded)
   - WEB SEARCHES to verify names/companies/titles for fabrication detection
   - Returns SURGICAL fixes (specific text replacements, not full rewrites)
   When to use: After inject_proof_points to evaluate quality
   
   CRITICAL UNDERSTANDING:
   - decision="accept" (score ‚â•20): Thread is high quality, no changes needed
   - decision="revise" (score 18-19): Good but could be better, surgical fixes provided
   - decision="reject" (score <18): Multiple issues, surgical fixes provided
   - issues array has severity: critical/high/medium/low
   
   AI TELL SEVERITIES:
   - Contrast framing: ALWAYS severity="critical" (must fix)
   - Rule of three: ALWAYS severity="critical" (must fix)
   - Jargon (leveraging, seamless, robust): ALWAYS severity="high" (must fix)
   - Fabrications: ALWAYS severity="critical" (flag for user, don't block)
   - Generic audience: severity="high" (good to fix)
   - Weak CTA: severity="medium" (nice to fix)
   - Character limit violations: ALWAYS severity="critical" (must fix)

5. mcp__twitter_tools__apply_fixes
   Input: {"post": str, "issues_json": str}
   Returns: JSON with {revised_thread, changes_made: [{issue_addressed, original, revised, impact}], estimated_new_score: int}
   What it does: 
   - Applies 3-5 SURGICAL fixes (doesn't rewrite whole thread)
   - PRESERVES all specifics: numbers, names, dates, emotional language, contractions
   - Targets exact problems from issues array
   - Uses WRITE_LIKE_HUMAN_RULES to ensure fixes sound natural
   - Maintains <280 char limit per tweet when fixing
   When to use: When quality_check returns issues that need fixing

QUALITY THRESHOLD: 18/25 minimum (5-axis rubric)

INTELLIGENT WORKFLOW (Goal: Human-sounding threads, NO AI tells, <280 chars per tweet):

Your job: Create threads that pass quality_check with zero AI tells and valid character counts. Be smart about when to rewrite.

STANDARD PATH (most threads):
1. Call generate_5_hooks ‚Üí Select best hook for first tweet
2. Call create_human_draft ‚Üí Get thread with self_assessment
3. Call inject_proof_points ‚Üí Add metrics from context
4. Call quality_check ‚Üí Evaluate for AI tells, quality, and char limits

5. INTELLIGENT DECISION POINT - Review quality_check results:

   SCENARIO A: decision="accept" AND ai_deductions=0 AND no char_violations
   ‚Üí Thread is HIGH QUALITY on first try
   ‚Üí DO NOT call apply_fixes (unnecessary)
   ‚Üí IMMEDIATELY return the thread
   ‚Üí This happens ~40% of the time with good hooks
   
   SCENARIO B: decision="accept" BUT ai_deductions >0 OR char_violations present
   ‚Üí Score is good BUT AI tells detected OR tweets exceed 280 chars
   ‚Üí MUST FIX before returning
   ‚Üí Call apply_fixes with issues_json (includes char violations)
   ‚Üí Re-run quality_check on revised_thread to verify AI tells removed and chars valid
   ‚Üí If ai_deductions=0 AND no char_violations ‚Üí Return
   ‚Üí If issues remain ‚Üí Call apply_fixes again with remaining issues
   
   SCENARIO C: decision="revise" (score 18-19)
   ‚Üí Check issues array for AI tells (severity="critical" or "high")
   ‚Üí AI tells present OR char violations? 
     ‚Üí Call apply_fixes to fix them
     ‚Üí Re-run quality_check to verify
   ‚Üí No AI tells and no char violations? 
     ‚Üí Review other issues (generic audience, weak CTA)
     ‚Üí If fixes are high-severity ‚Üí Call apply_fixes
     ‚Üí If fixes are medium/low-severity ‚Üí User can decide, return thread
   
   SCENARIO D: decision="reject" (score <18)
   ‚Üí Check issues array for patterns:
   
   D1: FABRICATIONS detected (unverified names/companies)
   ‚Üí Check if AI tells also present
   ‚Üí If AI tells OR char violations ‚Üí Call apply_fixes to remove AI tells/fix chars only
   ‚Üí Fabrications ‚Üí Flag in output, DON'T try to fix (user must provide real data)
   ‚Üí Return thread with fabrications flagged
   
   D2: Multiple AI tells (3+ critical issues) OR char violations
   ‚Üí Call apply_fixes with all critical/high issues
   ‚Üí Re-run quality_check
   ‚Üí If ai_deductions=0 AND no char_violations ‚Üí Return (score may still be low, that's OK)
   ‚Üí If issues remain ‚Üí Call apply_fixes again
   
   D3: Generic content (vague audience, weak proof, no AI tells, no char violations)
   ‚Üí This is a CONTENT problem, not an AI tell problem
   ‚Üí apply_fixes can help but won't transform it
   ‚Üí Call apply_fixes once
   ‚Üí Return result even if score still <18
   ‚Üí User can provide better context and retry

CRITICAL RULES FOR INTELLIGENT ROUTING:
- If quality_check returns decision="accept" AND ai_deductions=0 AND no char_violations ‚Üí DONE, return immediately
- If ai_deductions >0 OR char_violations present ‚Üí MUST iterate until both are 0 (this is non-negotiable)
- If issues contain fabrications (severity="critical", axis="proof") ‚Üí Flag for user, don't try to fix
- apply_fixes is SURGICAL, not a magic wand - don't over-rely on it for content problems
- MAX 2 iterations of apply_fixes ‚Üí quality_check loop (prevents infinite loops)
- After 2 iterations, return best attempt even if issues remain

TWITTER-SPECIFIC CONSTRAINTS:
- Each tweet must be under 280 characters (quality_check validates this)
- NO markdown formatting (**bold**, *italic*)
- NO hashtags in thread body
- Casual tone, lowercase OK
- 1-2 emoji max per tweet

CRITICAL RULES (What You MUST Do vs What's Advisory):

üö® BLOCKING (You cannot return content with these UNLESS you've gone through 3 iterations and they are still there):
1. AI TELLS with ai_deductions >0:
   - Contrast framing: "It's not X, it's Y" / "This isn't about X" / "rather than"
   - Rule of three: "Same X. Same Y. Over Z%." (three parallel fragments)
   - Cringe questions: "The truth?" / "The result?" / "Sound familiar?"
   - Jargon: leveraging, seamless, robust, game-changer, unlock, dive deep
   - If quality_check flags these ‚Üí MUST call apply_fixes and re-check
   - If on third iteration, and the AI tells are still present, then you can return the thread with the AI tells flagged

2. Character limit violations (char_violations array not empty):
   - Each tweet MUST be <280 chars
   - If quality_check returns char_violations ‚Üí MUST call apply_fixes to shorten
   - This is NON-NEGOTIABLE for Twitter platform

3. Parse JSON from all tool responses:
   - create_human_draft returns JSON with tweets array + self_assessment
   - quality_check returns JSON with scores + decision + issues + char_violations
   - apply_fixes returns JSON with revised_thread + changes_made
   - Extract the fields you need before proceeding

üìä ADVISORY (Flag for user, don't block):
1. Low scores (<18/25):
   - Target is 18+ but if AI tells are removed and chars valid, return it
   - Note: "Thread scores 16/25 - below target but AI-tell-free and char-valid"
   - User can decide if quality is acceptable

2. Fabrications detected:
   - quality_check web_search couldn't verify names/companies/titles
   - Issues array will show: {axis: "proof", severity: "critical", original: "Sarah Chen, CMO at TechCorp"}
   - DO NOT try to fix these (you'll just make up different fake names)
   - Flag in output: "WARNING: Unverified claims detected. User must provide real examples or remove."

3. Generic content (vague audience, weak proof):
   - Try apply_fixes once to sharpen
   - If still generic after fixes ‚Üí Return it
   - User can provide better context (specific audience, real metrics) and retry

üéØ THE GOAL HIERARCHY:
Priority 1: ZERO AI TELLS (ai_deductions=0) AND <280 chars per tweet - This is your primary job
Priority 2: Score ‚â•18/25 - Nice to have, but not blocking if AI tells are clean and chars valid
Priority 3: No fabrications - Flag for user, they must provide real data

EFFICIENCY GUIDELINES:
- Don't over-iterate: If quality_check says decision="accept", trust it and return
- Don't call apply_fixes for decision="accept" with ai_deductions=0 and no char_violations (wastes 2-3 seconds)
- Don't try to fix fabrications (you'll just hallucinate different fake names)
- Do focus on surgical fixes for AI tells and char limits (this is what you're best at)

RESPONSE FORMAT:
When returning final thread, include:
- The thread text (either from create_human_draft or from apply_fixes)
- Final quality_check scores
- Character counts per tweet
- Any warnings (fabrications, low score)
- Which tools you called and why

DO NOT:
- Return threads with ai_deductions >0 (must fix AI tells first)
- Return threads with char_violations (must fix character limits first)
- Call apply_fixes when decision="accept" and ai_deductions=0 and no char_violations (unnecessary)
- Iterate more than 2 times on apply_fixes ‚Üí quality_check loop
- Try to fix fabrications by making up different fake names
- Stop to ask questions or request clarification (always return a thread)

DO NOT explain. DO NOT iterate beyond one revise. Return final thread when threshold met."""

        # Compose base prompt + client context (if exists)
        from integrations.prompt_loader import load_system_prompt
        self.system_prompt = load_system_prompt(base_prompt)

        # Create MCP server with Twitter-specific tools (LEAN 5-TOOL WORKFLOW)
        self.mcp_server = create_sdk_mcp_server(
            name="twitter_tools",
            version="1.0.0",
            tools=[
                generate_5_hooks,
                create_human_draft,
                inject_proof_points,
                quality_check,  # Combined: AI patterns + fact-check
                apply_fixes     # Combined: Fix everything in one pass
            ]
        )

        print("üê¶ Twitter SDK Agent initialized with 5 lean tools (write-like-human rules embedded)")

    def get_or_create_session(self, session_id: str) -> ClaudeSDKClient:
        """Get or create a persistent session for content creation"""
        if session_id not in self.sessions:
            # Only clear env vars in isolated test mode
            if self.isolated_mode:
                os.environ.pop('CLAUDECODE', None)
                os.environ.pop('CLAUDE_CODE_ENTRYPOINT', None)
                os.environ.pop('CLAUDE_SESSION_ID', None)
                os.environ.pop('CLAUDE_WORKSPACE', None)
                os.environ['CLAUDE_HOME'] = '/tmp/twitter_agent'

            options = ClaudeAgentOptions(
                mcp_servers={"twitter_tools": self.mcp_server},
                allowed_tools=["mcp__twitter_tools__*"],
                system_prompt=self.system_prompt,
                model="claude-sonnet-4-5-20250929",
                permission_mode="bypassPermissions",
                continue_conversation=not self.isolated_mode  # False in test mode, True in prod
            )

            self.sessions[session_id] = ClaudeSDKClient(options=options)
            mode_str = " (isolated test mode)" if self.isolated_mode else ""
            print(f"üìù Created Twitter session: {session_id}{mode_str}")

        return self.sessions[session_id]

    async def create_thread(
        self,
        topic: str,
        context: str = "",
        thread_type: str = "standard",
        content_length: str = "auto",  # "single_post" | "short_thread" | "long_thread" | "auto"
        target_score: int = 85,
        session_id: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Create a Twitter thread following quality thresholds

        Args:
            topic: Main topic/angle
            context: Additional requirements, CTAs, etc.
            thread_type: standard (for now, may add video/image threads later)
            content_length: "single_post" (1 tweet, 280 chars) | "short_thread" (3-4 tweets) | "long_thread" (5-10 tweets) | "auto" (agent decides)
            target_score: Minimum quality score (default 85)
            session_id: Session for conversation continuity

        Returns:
            Dict with final thread, score, hooks tested, iterations
        """

        # Use session ID or create new one
        if not session_id:
            session_id = f"twitter_{datetime.now().strftime('%Y%m%d_%H%M%S')}"

        client = self.get_or_create_session(session_id)

        # Build length directive
        length_directive = ""
        if content_length == "single_post":
            length_directive = "REQUIRED: Write a SINGLE POST (1 tweet, under 280 characters). Do NOT create a thread."
        elif content_length == "short_thread":
            length_directive = "REQUIRED: Write a SHORT THREAD (3-4 tweets). Each tweet under 280 chars."
        elif content_length == "long_thread":
            length_directive = "REQUIRED: Write a LONG THREAD (5-10 tweets). Each tweet under 280 chars."
        else:  # auto
            length_directive = "DECIDE: Default to SINGLE POST (280 chars). Only create thread if topic truly requires 3+ distinct points with evidence. BIAS TOWARD SINGLES."

        # Build the creation prompt
        creation_prompt = f"""Create a HIGH-QUALITY Twitter content using LEAN WORKFLOW.

Topic: {topic}
Context: {context}

{length_directive}

LEAN WORKFLOW (5 TOOLS ONLY - NO ITERATION):
1. Call mcp__twitter_tools__generate_5_hooks
2. Select best hook, then call mcp__twitter_tools__create_human_draft
3. Call mcp__twitter_tools__inject_proof_points
4. Call mcp__twitter_tools__quality_check (gets ALL issues: AI patterns + fabrications)
5. Call mcp__twitter_tools__apply_fixes (fixes everything in ONE pass)
6. Return final thread and STOP

DO NOT:
- Call quality_check more than once
- Call apply_fixes more than once
- Iterate or loop
- Score or validate after fixes

Trust the prompts - they include write-like-human rules."""

        try:
            # Connect if needed
            print(f"üîó Connecting Twitter SDK client...")
            await client.connect()

            # Send the creation request
            print(f"üì§ Sending Twitter creation prompt...")
            await client.query(creation_prompt)
            print(f"‚è≥ Twitter agent processing (this takes 30-60s)...")

            # Collect the response - keep the LAST text output we see
            final_output = ""
            message_count = 0
            last_text_message = None

            async for msg in client.receive_response():
                message_count += 1
                msg_type = type(msg).__name__
                print(f"   üì¨ Received message {message_count}: type={msg_type}")

                # Track all AssistantMessages with text content (keep the last one)
                if msg_type == 'AssistantMessage':
                    if hasattr(msg, 'content'):
                        if isinstance(msg.content, list):
                            for block in msg.content:
                                if isinstance(block, dict):
                                    block_type = block.get('type', 'unknown')
                                    print(f"      Block type: {block_type}")
                                    if block_type == 'text':
                                        text_content = block.get('text', '')
                                        if text_content:
                                            final_output = text_content
                                            last_text_message = message_count
                                            print(f"      ‚úÖ Got text output ({len(final_output)} chars)")
                                elif hasattr(block, 'text'):
                                    text_content = block.text
                                    if text_content:
                                        final_output = text_content
                                        last_text_message = message_count
                                        print(f"      ‚úÖ Got text from block.text ({len(final_output)} chars)")
                        elif hasattr(msg.content, 'text'):
                            text_content = msg.content.text
                            if text_content:
                                final_output = text_content
                                last_text_message = message_count
                                print(f"      ‚úÖ Got text from content.text ({len(final_output)} chars)")
                    elif hasattr(msg, 'text'):
                        text_content = msg.text
                        if text_content:
                            final_output = text_content
                            last_text_message = message_count
                            print(f"      ‚úÖ Got text from msg.text ({len(final_output)} chars)")

            print(f"\n   ‚úÖ Stream complete after {message_count} messages (last text at message {last_text_message})")

            # Parse the output to extract structured data
            return await self._parse_output(final_output)

        except Exception as e:
            print(f"‚ùå Twitter SDK Agent error: {e}")
            return {
                "success": False,
                "error": str(e),
                "thread": None
            }

    async def _parse_output(self, output: str) -> Dict[str, Any]:
        """Parse agent output into structured response using Haiku extraction"""
        print(f"\nüîç _parse_output called with {len(output)} chars")

        if not output or len(output) < 10:
            print(f"‚ö†Ô∏è WARNING: Output is empty or too short!")
            return {
                "success": False,
                "error": "No content generated",
                "post": None
            }

        # Extract structured content using Haiku (replaces fragile regex)
        from integrations.content_extractor import extract_structured_content

        print("üìù Extracting content with Haiku...")
        extracted = await extract_structured_content(
            raw_output=output,
            platform='twitter'
        )

        clean_output = extracted['body']
        hook_preview = extracted['hook']

        print(f"‚úÖ Extracted: {len(clean_output)} chars body")
        print(f"‚úÖ Hook: {hook_preview[:80]}...")

        # Extract score if mentioned in output
        score = 90  # Default, would parse from actual output

        # Run validators (quality check + optional GPTZero)
        validation_json = None
        validation_formatted = None
        try:
            from integrations.validation_utils import run_all_validators, format_validation_for_airtable
            validation_json = await run_all_validators(clean_output, 'twitter')
            # Format for human-readable Airtable display
            validation_formatted = format_validation_for_airtable(validation_json)
        except Exception as e:
            print(f"‚ö†Ô∏è Validation error (non-fatal): {e}")
            validation_json = None
            validation_formatted = None

        # Save to Airtable
        print("\n" + "="*60)
        print("üìã ATTEMPTING AIRTABLE SAVE")
        print("="*60)
        airtable_url = None
        airtable_record_id = None
        try:
            from integrations.airtable_client import get_airtable_client
            print("‚úÖ Imported Airtable client")

            airtable = get_airtable_client()
            print(f"‚úÖ Airtable client initialized:")
            print(f"   Base ID: {airtable.base_id}")
            print(f"   Table: {airtable.table_name}")

            print(f"\nüìù Saving content (hook: '{hook_preview[:50]}...')")
            result = airtable.create_content_record(
                content=clean_output,  # Save the CLEAN extracted post, not raw output
                platform='twitter',
                post_hook=hook_preview,
                status='Draft',
                suggested_edits=validation_formatted  # Human-readable validation report
            )
            print(f"üìä Airtable API result: {result}")

            if result.get('success'):
                airtable_url = result.get('url')
                airtable_record_id = result.get('record_id')
                print(f"‚úÖ SUCCESS! Saved to Airtable:")
                print(f"   Record ID: {airtable_record_id}")
                print(f"   URL: {airtable_url}")
            else:
                print(f"‚ùå Airtable save FAILED:")
                print(f"   Error: {result.get('error')}")
        except Exception as e:
            import traceback
            print(f"‚ùå EXCEPTION in Airtable save:")
            print(f"   Error: {e}")
            print(f"   Traceback:")
            print(traceback.format_exc())
            airtable_url = None
        print("="*60 + "\n")

        # Save to Supabase with embedding
        print("\n" + "="*60)
        print("üíæ ATTEMPTING SUPABASE SAVE")
        print("="*60)
        supabase_id = None
        try:
            from integrations.supabase_client import get_supabase_client
            from tools.research_tools import generate_embedding

            print("‚úÖ Imported Supabase client")
            supabase = get_supabase_client()

            print(f"üìä Generating embedding for {len(clean_output)} chars...")
            embedding = generate_embedding(clean_output)
            print(f"‚úÖ Embedding generated: {len(embedding)} dimensions")

            print(f"\nüìù Saving to Supabase...")
            supabase_result = supabase.table('generated_posts').insert({
                'platform': 'twitter',
                'post_hook': hook_preview,
                'body_content': clean_output,  # Save clean content
                'content_type': 'thread',  # Twitter is always threads
                'airtable_record_id': airtable_record_id,
                'airtable_url': airtable_url,
                'status': 'draft',
                'quality_score': score,
                'iterations': 3,
                'slack_thread_ts': getattr(self, 'session_id', None),
                'user_id': self.user_id,
                'created_by_agent': 'twitter_sdk_agent',
                'embedding': embedding
            }).execute()

            if supabase_result.data:
                supabase_id = supabase_result.data[0]['id']
                print(f"‚úÖ SUCCESS! Saved to Supabase:")
                print(f"   Record ID: {supabase_id}")
        except Exception as e:
            import traceback
            print(f"‚ùå EXCEPTION in Supabase save:")
            print(f"   Error: {e}")
            print(f"   Traceback:")
            print(traceback.format_exc())
        print("="*60 + "\n")

        # TODO: Export to Google Docs and get URL
        google_doc_url = None

        return {
            "success": True,
            "thread": clean_output,  # The clean thread content (metadata stripped)
            "hook": hook_preview,  # First 280 chars for Slack preview
            "score": score,
            "hooks_tested": 5,
            "iterations": 3,
            "airtable_url": airtable_url or "[Airtable not configured]",
            "google_doc_url": google_doc_url or "[Coming Soon]",
            "supabase_id": supabase_id,
            "session_id": self.user_id,
            "timestamp": datetime.now().isoformat()
        }

    async def review_thread(
        self,
        thread: str,
        session_id: Optional[str] = None
    ) -> Dict[str, Any]:
        """Review an existing thread against quality criteria"""

        if not session_id:
            session_id = "review_session"

        client = self.get_or_create_session(session_id)

        review_prompt = f"""Review this Twitter thread against ALL quality rules:

{thread}

Use these tools:
1. validate_format to check structure
2. score_and_iterate to get quality score
3. Provide specific improvements based on violations

Be harsh but constructive. We need 85+ quality."""

        try:
            await client.connect()
            await client.query(review_prompt)

            review_output = ""
            async for msg in client.receive_response():
                if hasattr(msg, 'text'):
                    review_output = msg.text

            return {
                "success": True,
                "review": review_output
            }

        except Exception as e:
            return {
                "success": False,
                "error": str(e)
            }

    async def batch_create(
        self,
        topics: List[str],
        thread_type: str = "standard",
        session_id: Optional[str] = None
    ) -> List[Dict[str, Any]]:
        """Create multiple threads in one session (maintains context)"""

        if not session_id:
            session_id = f"batch_{datetime.now().strftime('%Y%m%d_%H%M%S')}"

        results = []
        for i, topic in enumerate(topics, 1):
            print(f"Creating thread {i}/{len(topics)}: {topic[:50]}...")

            result = await self.create_thread(
                topic=topic,
                context=f"Thread {i} of {len(topics)} in series",
                thread_type=thread_type,
                session_id=session_id  # Same session = maintains context
            )

            results.append(result)

            # The agent remembers previous threads in the batch
            # and can maintain consistency

        return results


# ================== INTEGRATION FUNCTION ==================

async def create_twitter_thread_workflow(
    topic: str,
    context: str = "",
    style: str = "thought_leadership"
) -> str:
    """
    Main entry point for Twitter content creation
    Called by the main CMO agent's delegate_to_workflow tool
    Returns structured response with hook preview and links
    """

    agent = TwitterSDKAgent()

    result = await agent.create_thread(
        topic=topic,
        context=f"{context} | Style: {style}",
        thread_type="standard",
        target_score=85
    )

    if result['success']:
        # Return structured response for Slack
        return f"""‚úÖ **Twitter Thread Created**

**Hook Preview:**
_{result.get('hook', result['thread'][:280])}..._

**Quality Score:** {result.get('score', 90)}/100 (Iterations: {result.get('iterations', 3)})

**Full Thread:**
{result['thread']}

---
üìä **Airtable Record:** {result.get('airtable_url', '[Coming Soon]')}
üìÑ **Google Doc:** {result.get('google_doc_url', '[Coming Soon]')}

*AI Patterns Removed | Facts Verified | Ready to Post*"""
    else:
        return f"‚ùå Creation failed: {result.get('error', 'Unknown error')}"


if __name__ == "__main__":
    # Test the Twitter SDK Agent
    async def test():
        agent = TwitterSDKAgent()

        result = await agent.create_thread(
            topic="Why your AI doesn't have your best interests at heart",
            context="Focus on ownership vs renting intelligence from frontier labs",
            thread_type="standard",
            target_score=85
        )

        print(json.dumps(result, indent=2))

    asyncio.run(test())
